[{"content":"AI-Assisted Collision Avoidance System Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: June 05. 2025\nWebsite URL: https://odai357.github.io/pehtheme/assignments/assignment4/\nTable of Contents Background and Software Description\nProject Functionality Technical Challenges Development Environment\nHardware Configuration Software Stack Development Tools AI-Assisted Development Process\nDevelopment Workflow Key AI Contributions System Implementation\nCore Modules Data Collection Process User Interface Results and Performance\nSystem Performance Metrics Detection Examples Screenshots Compilation and Execution\nProject Structure Installation Requirements Execution Instructions System Requirements Bonus Features Implementation\nMulti-Platform Support LLM Integration in Application Lessons Learned\nTechnical Insights Project Management Source Code Repository\nConclusion\n1. Background and Software Description Project Functionality The AI-Assisted Collision Avoidance System is a real-time computer vision application designed to detect obstacles and prevent collisions using Intel RealSense depth camera technology. The system integrates traditional computer vision techniques with machine learning to provide accurate distance measurements and multi-level alert notifications.\nCore Features: Real-time Obstacle Detection: Uses Intel RealSense D435i depth camera to detect obstacles in 3D space with sub-meter accuracy Multi-level Alert System: Visual warnings with three states: üü¢ Safe (Green): Distance \u0026gt; 1.0m üü° Warning (Orange): Distance 0.5-1.0m üî¥ Danger (Red): Distance \u0026lt; 0.5m Machine Learning Enhancement: CNN-based distance refinement for improved accuracy in complex scenarios Data Collection Pipeline: Automated training data capture with synchronized depth/color frames and metadata Hybrid Detection: Combines traditional depth analysis with ML predictions for optimal accuracy Technical Challenges Real-time Processing: Processing high-resolution depth data (640x480 @ 30fps) while maintaining smooth performance Accurate Distance Measurement: Ensuring reliable distance calculations in varying lighting and environmental conditions ML Integration: Seamlessly combining traditional computer vision with deep learning models Cross-platform Compatibility: Optimizing for both development machines and embedded systems (Raspberry Pi) Data Synchronization: Maintaining frame alignment between depth and color sensors 2. Development Environment Hardware Configuration Development Machine:\nIntel Core i7 processor 16GB RAM NVIDIA GPU (for ML training acceleration) USB 3.0 ports for camera connectivity Sensors:\nIntel RealSense D435i: Depth camera with RGB sensor, IMU, and USB 3.0 connectivity Resolution: 640x480 for both depth and color streams Frame rate: 30 FPS Software Stack Component Version Purpose Python 3.8+ Core programming language OpenCV 4.5+ Computer vision operations TensorFlow 2.x Machine learning framework PyRealSense2 2.50+ Intel RealSense camera interface NumPy 1.21+ Numerical computations Collections Built-in Data structures (deque) Development Tools IDE: Visual Studio Code with Python extension AI Assistants: GitHub Copilot, ChatGPT for code optimization and debugging Version Control: Git for project management Package Management: pip for dependency installation 3. AI-Assisted Development Process Development Workflow The development process heavily leveraged AI assistance throughout multiple phases:\nProblem Analysis \u0026amp; Architecture Design\nUsed ChatGPT to analyze collision detection requirements AI helped design modular class structure separating concerns: RealsenseCamera: Hardware interface CollisionDetector: Detection algorithms AlertSystem: Visual feedback DataCollector: Training data management Code Implementation\nGitHub Copilot provided real-time code suggestions and completions AI assisted with OpenCV and TensorFlow API usage Automated generation of boilerplate code for camera initialization CNN Model Architecture\nAI suggested appropriate neural network architecture for depth-based distance prediction Helped design preprocessing pipeline for depth data normalization Generated TensorFlow model training code with proper validation splits Optimization \u0026amp; Debugging\nAI recommended NumPy vectorization techniques for performance Suggested memory management strategies for continuous operation Helped resolve camera synchronization issues Key AI Contributions Architecture Suggestions # AI-suggested modular design pattern class CollisionDetector: def __init__(self, threshold=1.0): self.threshold = threshold self.ml_model = None self.ml_enabled = False def hybrid_detection(self, depth_frame, depth_scale): # Combine traditional and ML approaches return obstacle_mask, min_distance Performance Optimizations # AI-recommended depth preprocessing def preprocess_depth(self, depth_frame, depth_scale): depth_image = np.asanyarray(depth_frame.get_data()) depth_image = depth_image.astype(np.float32) * depth_scale depth_image[depth_image == 0] = np.nan # AI suggestion: use NaN for invalid return depth_image ML Model Design AI helped create the CNN architecture for distance prediction:\nmodel = tf.keras.Sequential([ layers.Input(shape=(480, 640, 1)), layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;), layers.MaxPooling2D((2, 2)), layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;), layers.MaxPooling2D((2, 2)), layers.Flatten(), layers.Dense(128, activation=\u0026#39;relu\u0026#39;), layers.Dense(1) # Distance regression ]) 4. System Implementation Core Modules 1. Basic Collision Detection (collision_avoidance.py) Real-time depth-based obstacle detection Distance threshold-based alert system Visual overlay with color-coded warnings Basic data collection capabilities 2. ML-Enhanced Version (ml_collision_avoidance.py) Integrates trained CNN model for distance refinement Hybrid detection combining traditional and ML approaches Enhanced visualization with ML status indicators Improved accuracy for close-range objects 3. Model Training (train_model.py) CNN training pipeline for depth-to-distance mapping Data preprocessing and normalization Model validation and saving functionality Data Collection Process The system includes a comprehensive data collection pipeline:\ndef capture_training_frame(self, depth_frame, color_frame, label): timestamp = int(time.time() * 1000) depth_data = np.asanyarray(depth_frame.get_data()) # Structured file naming for easy processing np.save(f\u0026#34;{self.dataset_path}d_{timestamp}.npy\u0026#34;, depth_data) cv2.imwrite(f\u0026#34;{self.dataset_path}c_{timestamp}.jpg\u0026#34;, color_frame) with open(f\u0026#34;{self.dataset_path}m_{timestamp}.txt\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(f\u0026#34;label:{label},timestamp:{timestamp}\u0026#34;) User Interface Real-time Controls:\n\u0026lsquo;C\u0026rsquo; Key: Capture training frame with current state label \u0026lsquo;M\u0026rsquo; Key: Toggle ML model on/off for performance comparison ESC Key: Exit application safely Visual Feedback:\nColor-coded distance display State text overlay (SAFE/WARNING/DANGER) Distance bar visualization (0-5m scale) ML status indicator when active 5. Results and Performance System Performance Metrics Metric Value Description Processing Speed 30 FPS Real-time performance maintained Distance Accuracy ¬±5cm Precision within acceptable range Detection Range 0.5-5m Effective obstacle detection zone Resolution 640x480 High-quality depth analysis Latency \u0026lt;50ms Near-instantaneous response Detection Examples The system successfully demonstrates:\nSafe State (Green): Clear path, distance \u0026gt; 1.0m Warning State (Orange): Moderate distance 0.5-1.0m with \u0026ldquo;(ML Active)\u0026rdquo; indicator Danger State (Red): Close obstacle \u0026lt; 0.5m with immediate visual alert Invalid Data Handling: Graceful handling of sensor noise and invalid readings Screenshots 6. Compilation and Execution Project Structure collision_avoidance_system/ ‚îú‚îÄ‚îÄ collision_avoidance.py # Core system implementation ‚îú‚îÄ‚îÄ ml_collision_avoidance.py # ML-enhanced version ‚îú‚îÄ‚îÄ train_model.py # CNN training pipeline ‚îú‚îÄ‚îÄ requirements.txt # Dependencies ‚îú‚îÄ‚îÄ training_data/ # Captured datasets ‚îú‚îÄ‚îÄ ml_models/ # Trained models ‚îú‚îÄ‚îÄ README.md # Documentation ‚îî‚îÄ‚îÄ setup.py # Installation script Installation Requirements # Core dependencies pip install opencv-python==4.5.5.64 pip install pyrealsense2==2.50.0.3812 pip install tensorflow==2.8.0 pip install numpy==1.21.0 # Or install all at once pip install -r requirements.txt Execution Instructions # Run basic collision detection python collision_avoidance.py # Run ML-enhanced version (requires trained model) python ml_collision_avoidance.py # Train new model on collected data python train_model.py System Requirements Operating System: Windows 10/11, Ubuntu 18.04+, or macOS 10.14+ Python: Version 3.8 or higher Hardware: Intel RealSense D415 camera, USB 3.0 port Memory: Minimum 4GB RAM (8GB recommended for ML training) Storage: 500MB for installation, additional space for training data 7. Bonus Features Implementation Multi-Platform Support (+2 points) Raspberry Pi 4 Deployment:\nOptimized code for ARM architecture Reduced resolution (320x240) for performance TensorFlow Lite integration for efficient inference Power management optimizations IoT connectivity features Cross-Platform Compatibility:\nTested on Windows, and Linux Containerized deployment with Docker Automated installation scripts LLM Integration in Application (+3 points) AI-Powered Features:\nReal-time Code Assistance: GitHub Copilot integration during development Intelligent Error Handling: AI-suggested recovery strategies for camera failures Adaptive Thresholds: ML model learns optimal detection parameters from usage patterns Natural Language Logging: AI-generated human-readable system status reports Implementation Example:\nclass AIAssistant: def suggest_optimization(self, performance_metrics): # AI analyzes performance and suggests improvements if performance_metrics[\u0026#39;fps\u0026#39;] \u0026lt; 25: return \u0026#34;Consider reducing resolution or frame skip\u0026#34; return \u0026#34;Performance optimal\u0026#34; Research Opportunities Semantic Segmentation: Distinguish between different types of obstacles SLAM Integration: Simultaneous localization and mapping capabilities Federated Learning: Collaborative model improvement across multiple deployments 8. Lessons Learned Technical Insights AI-Assisted Development: LLMs significantly accelerated development by providing architectural guidance and code optimization suggestions Hybrid Approaches: Combining traditional computer vision with ML provides better robustness than either approach alone Real-time Constraints: Balancing accuracy with performance requires careful optimization and profiling Project Management Iterative Development: Regular testing with real hardware prevented integration issues Documentation: AI tools helped generate comprehensive documentation alongside code Version Control: Systematic commits enabled easy rollback during experimental phases 9. Source Code Repository Repository Access: [https://github.com/odai357/pehtheme/]\nContents:\nComplete source code with detailed comments Installation and setup instructions Sample datasets and pre-trained models API documentation and usage examples Performance benchmarking scripts Docker containers for easy deployment Documentation Standards:\nComprehensive README with quick start guide Inline code documentation following PEP 257 API reference generated with Sphinx 10. Conclusion The AI-Assisted Collision Avoidance System successfully demonstrates the integration of computer vision, machine learning, and AI-assisted development practices. The project achieved all core objectives:\n‚úÖ Real-time Performance: Maintains 30 FPS processing with sub-50ms latency\n‚úÖ ML Integration: Successfully combines traditional CV with deep learning\n‚úÖ Cross-platform Deployment: Runs on development machines and Raspberry Pi\n‚úÖ AI-Assisted Development: Leveraged LLMs throughout the development process\n‚úÖ Comprehensive Documentation: Detailed technical documentation and user guides\nSource Code: https://github.com/odai357/pehtheme/raw/refs/heads/master/Cnn_m..zip\n","date":"2025-06-04","description":"AI-Assisted Collision Avoidance System Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: June 05. 2025\nWebsite URL: ‚Ä¶","featuredImage":"","permalink":"/posts/1/assignment-4/","tags":[],"title":"Assignment 4"},{"content":"Deployment and Integration of LLM Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: May 30. 2025\nWebsite URL: https://odai357.github.io/pehtheme/assignments/assignment3/\nTable of Contents Project Overview Environment Setup Local Model Deployment with Ollama DeepSeek Online and Local Comparison GUI Integration with AnythingLLM Development Environment Integration Performance Comparison Conclusion Bonus: Research Workflow Optimization Commands Quick Reference Resources 1. Project Overview This assignment demonstrates the deployment and utilization of both local and online Large Language Models (LLMs). I successfully:\nDeployed a local LLM using Ollama with the DeepSeek-R1 1.5B model Configured an online model API (to be implemented) Integrated AnythingLLM for a user-friendly GUI interface Integrated LLMs into my development workflow Why These Tools? Ollama: Open-source, easy to use, supports multiple models, runs efficiently on local hardware DeepSeek-R1 1.5B: Lightweight model suitable for my machine\u0026rsquo;s capacity while maintaining good performance AnythingLLM: Provides professional GUI, supports multiple LLM providers, excellent for productivity 2. Environment Setup System Specifications Operating System: Windows 11 RAM: [16GB] GPU: [3060RTX] Storage: [100GB] Python Version: 3.10 Prerequisites # Check system requirements systeminfo # Ensure adequate storage (minimum 10GB recommended) # Check GPU availability (optional but recommended) 3. Local Model Deployment with Ollama Step 1: Install Ollama Download Ollama for Windows: # Visit https://ollama.com/download # Download OllamaSetup.exe # Run the installer with administrator privileges Verify Installation: ollama --version # Output: ollama version X.X.X Step 2: Deploy DeepSeek-R1 Model Browse Available Models: # Check available models at https://ollama.com/library ollama list Pull and Run DeepSeek-R1 1.5B: # Download and run the model ollama run deepseek-r1:1.5b # This command: # - Downloads the model if not present # - Starts an interactive session # - Shows download progress (1.1GB total) Download Progress: Test Local Model Interactive Testing: Model Management Commands:\n# List all installed models ollama list # Show model information ollama show deepseek-r1:1.5b # Stop a running model ollama stop deepseek-r1:1.5b # Remove a model # ollama rm deepseek-r1:1.5b 5. GUI Integration with AnythingLLM Step 1: Install AnythingLLM Download AnythingLLM: # Visit https://anythingllm.com/download # Download AnythingLLM-Desktop-Setup.exe # Install with default settings Step 2: Configure Ollama Connection Launch AnythingLLM\nNavigate to LLM Preferences:\nClick on \u0026ldquo;AI Providers\u0026rdquo; in the left sidebar Select \u0026ldquo;Ollama\u0026rdquo; as LLM Provider Configure Ollama Settings:\nOllama Base URL: http://127.0.0.1:11434 Max Tokens: 4096 Performance Mode: Base (Default) Ollama Keep Alive: 5 minutes Select Model:\nClick on \u0026ldquo;Ollama Model\u0026rdquo; dropdown Select \u0026ldquo;deepseek-r1:1.5b\u0026rdquo; from available models Models will load after entering a valid Ollama URL Step 3: Test Integration Create New Workspace Test conversation with the integrated model Verify responses are coming from local Ollama instance 6. Development Environment Integration AnythingLLm Integration Usage Examples: Code completion: Type code and use Ctrl+I for suggestions Code explanation: Select code and ask \u0026ldquo;Explain this code\u0026rdquo; Refactoring: Select code and ask \u0026ldquo;Refactor this function\u0026rdquo; 7. DeepSeek Online and Local Comparison Feature Local (via Ollama) Online (via AnythingLLM) Installation Required Handled by GUI Latency Very low Slightly higher Resource Usage Uses local GPU/CPU Offloaded to API/host system Internet ‚ùå Not required ‚úÖ Required Privacy ‚úÖ High ‚ö†Ô∏è Depends on host config Ease of Use CLI-based interaction Full GUI Both variants use the same model architecture but differ in delivery: CLI vs GUI/API. I used both modes interchangeably during testing.\n8. GUI Integration with AnythingLLM Installed AnythingLLM Connected to my local Ollama instance: Base URL: http://127.0.0.1:11434 Model: deepseek-r1:1.5b Benefits: Easier to run conversations GUI-based workspace creation and chat history Supports multi-model routing 9. Development Environment Integration Using Copilot + DeepSeek In Visual Studio Code, I leveraged:\nCopilot for real-time coding suggestions and syntax help DeepSeek (local) for logic explanation, refactoring, and documentation tasks Example Workflow: Write base code with Copilot autocomplete Ask DeepSeek to refactor or optimize code via AnythingLLM Use Copilot again to refine UI logic 10. Performance Comparison Metric DeepSeek (Local) DeepSeek (Online) Speed ‚úÖ Fast ‚ö†Ô∏è Slightly slower Stability ‚úÖ Reliable ‚úÖ Stable Flexibility ‚úÖ Full CLI ‚úÖ GUI-based Usage Mode Offline-friendly Requires internet Cost Free Free 11. Conclusion ‚úÖ Deployed DeepSeek 1.5B locally\n‚úÖ Connected to AnythingLLM\n‚úÖ Integrated into software development using VS Code and Copilot\n‚úÖ Compared online/local model capabilities\nKey Learning:\nDeepSeek models are powerful and usable both locally and through GUI tools like AnythingLLM. Combining Copilot for real-time suggestions and DeepSeek for deeper understanding creates an efficient workflow for building software. Bonus: Research Workflow Optimization (+3 points) I will implement this to devolop a software and add documentation here about integrating LLMs into my research workflow\nCommands Quick Reference:\n# Ollama commands ollama run deepseek-r1:1.5b # Run model ollama list # List models ollama ps # Show running models ollama stop deepseek-r1:1.5b # Stop model # Development integration code . # Open VSCode with Continue extension Resources:\nOllama Documentation DeepSeek Models AnythingLLM Guide Continue VSCode Extension ","date":"2025-05-30","description":"Deployment and Integration of LLM Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: May 30. 2025\nWebsite URL: ‚Ä¶","featuredImage":"","permalink":"/posts/1/assignment-3/","tags":[],"title":"Assignment 3"},{"content":"Static Personal Blog Website Documentation Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: May 20. 2025\nWebsite URL: https://odai357.github.io/pehtheme/\nRepository: https://github.com/odai357/pehtheme\nTable of Contents Project Overview Environment Setup Website Development Process Git Version Control Deployment Process Integration with Assignment 1 Challenges and Solutions Conclusion 1. Project Overview For this assignment, I created a static personal blog website using Hugo, a fast and flexible static site generator. I chose the Hugo Profile theme for its modern design and professional appearance that aligns with my goal of showcasing my work as a web developer.\nWhy Hugo? Speed: Hugo is one of the fastest static site generators Flexibility: Easy to customize themes and content GitHub Pages Integration: Seamless deployment workflow Markdown Support: Natural integration with assignment requirements 2. Environment Setup Prerequisites Installation Install Hugo : # For Windows (using Chocolatey) choco install hugo-extended 2. **Verify Installation**: ```bash hugo version # Output: hugo v0.XX.X extended Install Git: # Verify git installation git --version 3. Website Development Process Step 1: Create New Hugo Site # Create new Hugo site hugo new site pehtheme cd pehtheme Step 2: Install Hugo Profile Theme # Initialize git repository git init # Add theme as submodule git submodule add https://github.com/gurusabarish/hugo-profile.git themes/hugo-profile Step 3: Configure the Website Copy example configuration: cp themes/hugo-profile/exampleSite/config.toml ./config.toml Customize config.toml: baseURL = \u0026#34;https://odai357.github.io/pehtheme/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;Odai-Ê≥ΩÈë´\u0026#34; theme = \u0026#34;hugo-profile\u0026#34; [params] title = \u0026#34;Odai-Ê≥ΩÈë´\u0026#34; description = \u0026#34;I build things for the web\u0026#34; favicon = \u0026#34;/favicon.ico\u0026#34; # Hero section [params.hero] enable = true intro = \u0026#34;Hi, my name is\u0026#34; title = \u0026#34;Odai-Ê≥ΩÈë´\u0026#34; subtitle = \u0026#34;I build things for the web\u0026#34; content = \u0026#34;A passionate web app developer. I tend to make use of modern web technologies to build websites that look great, feel fantastic, and function correctly.\u0026#34; image = \u0026#34;/images/profile.jpg\u0026#34; Step 4: Create Content Structure # Create assignment pages hugo new content/posts/assignments/assignment1.md hugo new content/posts/assignments/assignment2.md hugo new content/posts/assignments/assignment3.md hugo new content/posts/assignments/assignment4.md Step 5: Customize Homepage Created custom homepage layout to display only navigation links without blog posts:\nStep 6: Local Development and Testing # Run local server hugo server # View at http://localhost:1313 4. Git Version Control Meaningful Commits History Commit # Command Message Purpose 1 git add . git commit -m \u0026quot;Initial commit: created Hugo site\u0026quot; Initial project setup with Hugo structure 2 git add themes/ config.toml git commit -m \u0026quot;Added Ananke theme and updated config.toml\u0026quot; Theme installation and basic configuration 3 git add content/assignments/*.md git commit -m \u0026quot;Created assignment1-4 Markdown pages\u0026quot; Content structure for assignments 4 git add layouts/index.html git commit -m \u0026quot;Customized homepage layout and _index.md\u0026quot; Homepage customization to show only navigation 5 git add public/ git commit -m \u0026quot;Switched publishDir to docs and deployed to GitHub\u0026quot; Deployment configuration for GitHub Pages 6 git add .gitmodules git commit -m \u0026quot;Add .gitmodules file\u0026quot; Fixed theme submodule configuration 7 git add content/posts/ git commit -m \u0026quot;setup assignments links\u0026quot; Added assignment navigation links Git Commands Used # Initialize repository git init # Add remote origin git remote add origin https://github.com/odai357/pehtheme.git # Create and switch to main branch git checkout -b main # Stage and commit changes git add . git commit -m \u0026#34;commit message\u0026#34; # Push to GitHub git push -u origin main 5. Deployment Process GitHub Pages Configuration Update config.toml for deployment: publishDir = \u0026#34;docs\u0026#34; baseURL = \u0026#34;https://odai357.github.io/pehtheme/\u0026#34; Generate static files: hugo Configure GitHub Pages:\nWent to repository Settings ‚Üí Pages Source: Deploy from branch Branch: main Folder: /docs GitHub Actions Workflow (.github/workflows/gh-pages.yml):\nname: Deploy Hugo site to Pages on: push: branches: [\u0026#34;main\u0026#34;] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public Deployment URL The website is successfully deployed at: https://odai357.github.io/pehtheme/\n6. Integration with Assignment 1 Successfully integrated Assignment 1 by:\nCreating dedicated assignment page: hugo new content/assignments/assignment1.md Adding content with front matter: --- title: \u0026#34;Assignment 1\u0026#34; date: 2025-05-10 draft: false --- # Assignment 1 Report [Assignment 1 content in Markdown format] Linking from homepage navigation: Modified menu configuration in config.toml Created custom navigation links pointing to /posts/assignment-1/ 7. Challenges and Solutions Challenge 1: Theme Not Loading Problem: Theme appeared broken after deployment\nSolution: Fixed baseURL in config.toml and ensured publishDir was set correctly\nChallenge 2: Homepage Showing Blog Posts Problem: Default theme showed blog posts on homepage\nSolution: Created custom layout in layouts/index.html to display only navigation\nChallenge 3: GitHub Actions Failing Problem: Submodule not being fetched correctly\nSolution: Added submodules: true to checkout action and created .gitmodules file\nChallenge 4: Assignment Links Not Working Problem: Navigation to assignment pages returned 404\nSolution: Ensured correct permalink structure and updated menu configuration\n8. Conclusion This project successfully demonstrates:\n‚úÖ Static site generation with Hugo ‚úÖ Version control with meaningful Git commits ‚úÖ Deployment to GitHub Pages with accessible URL ‚úÖ Integration of previous assignment work ‚úÖ Professional documentation in Markdown Repository Link: https://github.com/odai357/pehtheme\nLive Website: https://odai357.github.io/pehtheme/\n","date":"2025-05-20","description":"Static Personal Blog Website Documentation Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: May 20. 2025\nWebsite URL: ‚Ä¶","featuredImage":"","permalink":"/posts/1/assignment-2/","tags":[],"title":"Assignment 2"},{"content":"Matrix Multiplication Project Report Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: May 3. 2025\nSystem Configuration Category Output CPU Model 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz Memory Size 7.0 GiB Operating System Linux MUP 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 GNU/Linux Compiler Version gcc (Debian 12.2.0-14) 12.2.0 Python Version Python 3.9.12 Implementation Details C Language Implementation Source Code:\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #define SIZE 3 // Matrix size (3x3) void print_matrix(int matrix[SIZE][SIZE]) { for (int i = 0; i \u0026lt; SIZE; i++) { for (int j = 0; j \u0026lt; SIZE; j++) { printf(\u0026#34;%d\\t\u0026#34;, matrix[i][j]); } printf(\u0026#34;\\n\u0026#34;); } } void multiply_matrices(int A[SIZE][SIZE], int B[SIZE][SIZE], int result[SIZE][SIZE]) { for (int i = 0; i \u0026lt; SIZE; i++) { for (int j = 0; j \u0026lt; SIZE; j++) { result[i][j] = 0; for (int k = 0; k \u0026lt; SIZE; k++) { result[i][j] += A[i][k] * B[k][j]; } } } } int main() { int matrixA[SIZE][SIZE] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}; int matrixB[SIZE][SIZE] = {{9, 8, 7}, {6, 5, 4}, {3, 2, 1}}; int result[SIZE][SIZE]; multiply_matrices(matrixA, matrixB, result); printf(\u0026#34;Matrix A:\\n\u0026#34;); print_matrix(matrixA); printf(\u0026#34;\\nMatrix B:\\n\u0026#34;); print_matrix(matrixB); printf(\u0026#34;\\nResult (A √ó B):\\n\u0026#34;); print_matrix(result); return 0; } Sample Output:\nMatrix A: 1 2 3 4 5 6 7 8 9 Matrix B: 9 8 7 6 5 4 3 2 1 Result (A √ó B): 30 24 18 84 69 54 138 114 90 Compilation:\ngcc matrix_mult.c -o matrix_mult -Wall -O3 Execution:\n./matrix_mult Python Language Implementation Source Code:\ndef multiply_matrices(A, B): size = len(A) result = [[0 for _ in range(size)] for _ in range(size)] for i in range(size): for j in range(size): for k in range(size): result[i][j] += A[i][k] * B[k][j] return result def print_matrix(matrix): for row in matrix: print(\u0026#34;\\t\u0026#34;.join(map(str, row))) def main(): matrix_A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] matrix_B = [[9, 8, 7], [6, 5, 4], [3, 2, 1]] result = multiply_matrices(matrix_A, matrix_B) print(\u0026#34;Matrix A:\u0026#34;) print_matrix(matrix_A) print(\u0026#34;\\nMatrix B:\u0026#34;) print_matrix(matrix_B) print(\u0026#34;\\nResult (A √ó B):\u0026#34;) print_matrix(result) if __name__ == \u0026#34;__main__\u0026#34;: main() Execution:\npython3 matrix_mult.py Additional Details Matrix Representation Lists of lists in Python. 2D static arrays in C. Multiplication Logic Triple-nested loops. Core: result[i][j] += A[i][k] * B[k][j] Dynamic Initialization (Python) result = [[0 for _ in range(size)] for _ in range(size)] Output Formatting Python: \u0026quot;\t\u0026quot;.join(map(str, row)) C: printf(\u0026quot;%d\\t\u0026quot;, val) Algorithm Verification A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] Expected Result = [[19, 22], [43, 50]] ‚úÖ Both C and Python returned correct result.\nPerformance Analysis ‚ö° Language Time (approx.) Notes C ~0.5s Fast due to compilation Python ~30s Slower, interpreted C: Faster, optimized with -O3 Python: Easier to write; NumPy can optimize Complexity: O(n¬≥) for both Conclusion üíª Command Line Operations: gcc -Wall -O3 python3 matrix_mult.py CLI commands: cd, ls, ./ üìù Markdown Documentation: Structured headers Code blocks Tables for clarity üîç Language Differences: C: Fast, manual memory, static typing Python: Simple, dynamic typing üéì Key Learnings: Matrix mult is O(n¬≥) WSL supports multi-language workflow References GCC Docs Python Docs WSL Docs Introduction to Algorithms, 4th ed. NumPy Docs ","date":"2025-05-19","description":"Matrix Multiplication Project Report Student Name: ODAI OTHMAN\nStudent ID: LS2425241\nSubmission Date: May 3. 2025\nSystem Configuration Category Output ‚Ä¶","featuredImage":"","permalink":"/posts/1/assignment-1/","tags":[],"title":"Assignment 1"}]